---
layout: default
title: Jiateng Liu, Personal Home Page
---
<div class="blurb">
	
	<!-- Header Section with Photo and Greeting -->
	<div class="header-section">
		<div class="header-content">
			<div class="profile-photo-container" onclick="toggleProfilePhoto()">
				<img class="profile-photo-main" alt="profile photo" src="images/profile.jpg">
				<img class="profile-photo-hover" alt="profile photo hover" src="images/image2.jpg">
			</div>
			<div class="greeting-section">
				<h2 class="greeting">Hi, I'm Jiateng Liu.</h2>
			</div>
		</div>
	</div>

	<!-- Introduction Section -->
	<div class="introduction-section">
		<p>I'm Jiateng Liu (刘嘉腾), a second-year MS student at the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign (UIUC)</a> under the guidance of Prof. <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>. Previously, I earned my bachelor's degree in Computer Science from <a href="https://www.zju.edu.cn/">Zhejiang University</a>.</p>

		<p>I'm interested in the <strong>Science</strong> of NLP while keeps an eye on its <strong>Applications</strong>.</p>

		<p><strong>Science:</strong> I am deeply committed to the rigorous study of large models. I strive to enhance their interpretability and effectiveness. As we move into the next decade, I am excited to contribute to the evolution of AI—developing systems that not only perform complex tasks but are also interpretable and genuinely beneficial to humanity.</p>
		
		<p><strong>Applications:</strong> I believe that AI must transcend theoretical prowess to substantial improvements in daily life. I keep an eye on the applications of NLP in Science and looking forward to the next 'ChatGPT' moment.</p>
	</div>

  <!-- Research Section -->
  <div class="research-section">
    <h2>Research</h2>
    <p class="research-intro">My research interests are organized into three key areas:</p>
    
    <!-- Research Area 1 -->
    <div class="research-card">
      <div class="research-image">
        <img src="images/1F.jpg" alt="The Physics and Interpretability of Language Models" onclick="openImageModal('images/1F.jpg')">
      </div>
      <div class="research-content">
        <div class="research-tags">
          <span class="research-tag">Language Models</span>
          <span class="research-tag">Interpretability</span>
        </div>
        <h3 class="research-title">The Physics and Interpretability of Language Models</h3>
        <div class="research-description">
          <p>I am intrigued by the underlying "physics" of Large Language Models (LLMs), focusing on how they absorb knowledge, process information, and make predictions. A major part of my research is improving the efficiency and accuracy of updating pretrained LLMs, ensuring that their knowledge remains robust, consistent, and up-to-date while minimizing costs and time.</p>
          <p>Additionally, I am deeply interested in the interpretability of LLMs—understanding how they represent and manipulate information internally to provide more transparent and trustworthy AI systems.</p>
        </div>
      </div>
    </div>

    <!-- Research Area 2 -->
    <div class="research-card">
      <div class="research-image">
        <img src="images/2F.jpg" alt="Multi-Modal Representation Learning" onclick="openImageModal('images/2F.jpg')">
      </div>
      <div class="research-content">
        <div class="research-tags">
          <span class="research-tag">Multimodal AI</span>
          <span class="research-tag">Video Understanding</span>
        </div>
        <h3 class="research-title">Multi-Modal Representation Learning and Multi-Media Foundational Models</h3>
        <div class="research-description">
          <p>My work in this area centers on designing new paradigms for multi-modal interactions and deriving empirical scaling laws for multi-modal foundational models. I focus particularly on video understanding and generation, aiming to seamlessly integrate language, visual, and temporal modalities.</p>
          <p>A key aspect of my research is exploring how multimodal interactions are learned, including the mechanisms by which information flows and aligns across modalities to create cohesive representations. I also investigate protocols for efficient and complete multimodal interactions, ensuring that each modality contributes optimally to the task at hand while minimizing redundancy and maximizing interpretability.</p>
          <p>My goal is to develop systems capable of effectively analyzing and generating content across diverse modalities, with applications in tasks such as video captioning, video-based reasoning, and video synthesis, thereby advancing the capabilities of multi-modal AI.</p>
        </div>
      </div>
    </div>

    <!-- Research Area 3 -->
    <div class="research-card">
      <div class="research-image">
        <img src="images/3F.jpg" alt="NLP for Social Good" onclick="openImageModal('images/3F.jpg')">
      </div>
      <div class="research-content">
        <div class="research-tags">
          <span class="research-tag">Social Good</span>
          <span class="research-tag">Interdisciplinary</span>
        </div>
        <h3 class="research-title">NLP and Multi-Disciplinary Science for Social Good</h3>
        <div class="research-description">
          <p>I use NLP to tackle challenges in social and scientific domains. For example, I analyze social media data to study the spread of misinformation and its societal impacts, helping to develop tools that counteract disinformation.</p>
          <p>Additionally, I explore how NLP can contribute to scientific advancements, such as facilitating drug design and interpreting neural signals. These interdisciplinary applications demonstrate the transformative potential of NLP in both improving lives and advancing science.</p>
        </div>
      </div>
    </div>

    <!-- Research Footer -->
    <div class="research-footer">
      <p>For more on my future vision and projects, feel free to visit my blog or reach out for a discussion.</p>
    </div>
  </div>


  <div class="publications-section">
    <h2>Publications</h2>
    
    <div style="margin-bottom: 30px; text-align: center; font-style: italic; color: #6b7280;">
      For the full list of publications, please visit my <a href="https://scholar.google.com/citations?user=a5_PfQQAAAAJ&hl" style="color: #3b82f6; text-decoration: none;">Google Scholar page</a>.
    </div>
    
    <!-- Publication 1: EVEDIT -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/1.jpg" alt="EVEDIT: Event-based Knowledge Editing" onclick="openImageModal('images/1.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag emnlp">EMNLP-2024</span>
          <span class="publication-tag">Knowledge Editing</span>
        </div>
        <h3 class="publication-title">EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Pengfei Yu*, Yuji Zhang, Sha Li, Zixuan Zhang, Ruhi Sarikaya, Kevin Small, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP-2024 Main Conference)</div>
        <div class="publication-links">
          <a href="https://aclanthology.org/2024.emnlp-main.282/" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 2: Procedure Planning -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/2.jpg" alt="A Language First Approach for Procedure Planning" onclick="openImageModal('images/2.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag acl">ACL-2023</span>
          <span class="publication-tag">Procedure Planning</span>
        </div>
        <h3 class="publication-title">A Language First Approach for Procedure Planning</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Sha Li*, Zhenhailong Wang, Manling Li, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023 Findings)</div>
        <div class="publication-links">
          <a href="https://aclanthology.org/2023.findings-acl.122.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 3: PropaInsight -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/3.jpg" alt="PropaInsight: Toward Deeper Understanding of Propaganda" onclick="openImageModal('images/3.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag coling">COLING-2025</span>
          <span class="publication-tag">Propaganda Analysis</span>
        </div>
        <h3 class="publication-title">PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Lin Ai*, Zizhou Liu, Payam Karisani, Zheng Hui, May Fung, Preslav Nakov, Julia Hirschberg, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 29th International Conference on Computational Linguistics (COLING 2025)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2409.18997" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 4: Code Survey -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/5.jpg" alt="If LLM Is the Wizard, Then Code Is the Wand" onclick="openImageModal('images/5.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag iclr">ICLR-2024</span>
          <span class="publication-tag">Survey</span>
        </div>
        <h3 class="publication-title">If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents</h3>
        <div class="publication-authors">Ke Yang*, <strong>Jiateng Liu*</strong>, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai</div>
        <div class="publication-venue">ICLR 2024 Workshop (submitting to ACM Computing Survey)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2401.00812.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 5: MINT -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/4.jpg" alt="MINT: Evaluating LLMs in Multi-turn Interaction" onclick="openImageModal('images/4.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag iclr">ICLR-2024</span>
          <span class="publication-tag">LLM Evaluation</span>
        </div>
        <h3 class="publication-title">MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</h3>
        <div class="publication-authors">Xingyao Wang, Zihan Wang, <strong>Jiateng Liu</strong>, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 12th International Conference on Learning Representations (ICLR 2024)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2309.10691.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 6: CurveCloudNet -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/6.jpg" alt="CurveCloudNet: Processing Point Clouds" onclick="openImageModal('images/6.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag cvpr">CVPR-2024</span>
          <span class="publication-tag">3D Vision</span>
        </div>
        <h3 class="publication-title">CurveCloudNet: Processing Point Clouds with 1D Structure</h3>
        <div class="publication-authors">Colton Stearns, <strong>Jiateng Liu</strong>, Davis Rempe, Despoina Paschalidou, Jeong Joon Park, Sebastien Mascha, Leonidas J. Guibas</div>
        <div class="publication-venue">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2303.12050.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 7: Knowledge Overshadowing -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/7.jpg" alt="Knowledge overshadowing causes amalgamated hallucination" onclick="openImageModal('images/7.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag arxiv">arXiv</span>
          <span class="publication-tag">Hallucination</span>
        </div>
        <h3 class="publication-title">Knowledge overshadowing causes amalgamated hallucination in large language models</h3>
        <div class="publication-authors">Yuji Zhang, Sha Li, <strong>Jiateng Liu</strong>, Pengfei Yu, Yi R Fung, Jing Li, Manling Li, Heng Ji</div>
        <div class="publication-venue">arXiv preprint</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2407.08039" class="publication-link">Paper</a>
        </div>
      </div>
    </div>
  </div>

  <!-- Research Experiences Section -->
  <div class="research-experiences-section">
    <h2>Research Experiences</h2>
    
    <!-- Stanford University -->
    <div class="research-experience-card">
      <div class="research-experience-header">
        <a href="https://www.stanford.edu/" class="institution-name">Stanford University</a>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Advisor:</span>
        <span class="advisor-name">Prof. Leonidas Guibas, Prof. Yanchao Yang, Colton Stearns</span>
      </div>
      <div class="research-experience-dates">Nov 2022 - March 2023</div>
      <div class="research-experience-focus">
        <ul>
          <li>3D reconstruction with curve data</li>
        </ul>
      </div>
    </div>

    <!-- University of Illinois Urbana Champaign -->
    <div class="research-experience-card">
      <div class="research-experience-header">
        <a href="https://illinois.edu/" class="institution-name">University of Illinois Urbana Champaign</a>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Advisor:</span>
        <span class="advisor-name">Prof. Heng Ji, Sha Li, Manling Li</span>
      </div>
      <div class="research-experience-dates">June 2022 - Dec 2022</div>
      <div class="research-experience-focus">
        <ul>
          <li>Language side approaches for Procedure Planning</li>
        </ul>
      </div>
    </div>

    <!-- Zhejiang University (First) -->
    <div class="research-experience-card">
      <div class="research-experience-header">
        <a href="https://www.zju.edu.cn/" class="institution-name">Zhejiang University</a>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Advisor:</span>
        <span class="advisor-name">Prof. Mingli Song, Prof. Zunlei Feng, Ya Zhao</span>
      </div>
      <div class="research-experience-dates">June 2022 - Dec 2022</div>
      <div class="research-experience-focus">
        <ul>
          <li>Make Transformers efficient</li>
        </ul>
      </div>
    </div>

    <!-- Zhejiang University (Second) -->
    <div class="research-experience-card">
      <div class="research-experience-header">
        <a href="https://www.zju.edu.cn/" class="institution-name">Zhejiang University</a>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Advisor:</span>
        <span class="advisor-name">Prof. Zicheng Liu, Prof. Mingli Song</span>
      </div>
      <div class="research-experience-dates">Sep 2021 - Dec 2021</div>
      <div class="research-experience-focus">
        <ul>
          <li>3D Human mesh reconstruction</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Work Experience Section -->
  <div class="work-experience-section">
    <h2>Work Experience</h2>
    
    <!-- Amazon Alexa AI -->
    <div class="work-experience-card">
      <div class="work-experience-header">
        <span class="company-name">Amazon</span>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Team:</span>
        <span class="team-name">Alexa AI</span>
      </div>
      <div class="work-experience-position">Applied Scientist Intern</div>
      <div class="work-experience-location">Seattle, WA</div>
      <div class="work-experience-focus">
        <ul>
          <li>Applied Scientist Intern at Amazon Alexa AI team</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Assistantship Section -->
  <div class="assistantship-section">
    <h2>Assistantship</h2>
    
    <!-- Teaching Assistant -->
    <div class="assistantship-card">
      <div class="assistantship-header">
        <span class="position-name">Teaching Assistant</span>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Course:</span>
        <span class="course-name">CS440 at UIUC</span>
      </div>
      <div class="assistantship-dates">Aug 2023 - Dec 2023</div>
      <div class="assistantship-focus">
        <ul>
          <li>Teaching assistant for CS440 at UIUC</li>
        </ul>
      </div>
    </div>

    <!-- Research Assistant -->
    <div class="assistantship-card">
      <div class="assistantship-header">
        <span class="position-name">Research Assistant</span>
        <span class="advisor-label">|</span>
        <span class="advisor-label">Advisor:</span>
        <span class="advisor-name">Prof. Heng Ji</span>
      </div>
      <div class="assistantship-dates">Dec 2023 - Present</div>
      <div class="assistantship-focus">
        <ul>
          <li>Research assistant of Prof. Heng Ji</li>
        </ul>
      </div>
    </div>
  </div>
	
	</div><!-- /.blurb -->

<!-- Image Modal -->
<div id="imageModal" class="image-modal" onclick="closeImageModal()">
    <span class="image-modal-close" onclick="closeImageModal()">&times;</span>
    <img class="image-modal-content" id="modalImage">
</div>

<script>
function openImageModal(imageSrc) {
    const modal = document.getElementById('imageModal');
    const modalImg = document.getElementById('modalImage');
    modal.style.display = 'block';
    modalImg.src = imageSrc;
}

function closeImageModal() {
    document.getElementById('imageModal').style.display = 'none';
}

// Close modal when clicking outside the image
document.getElementById('imageModal').addEventListener('click', function(e) {
    if (e.target === this) {
        closeImageModal();
    }
});

// Close modal with Escape key
document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape') {
        closeImageModal();
    }
});

// Toggle profile photo on click
function toggleProfilePhoto() {
    const container = document.querySelector('.profile-photo-container');
    container.classList.toggle('show-hover');
}
</script>
