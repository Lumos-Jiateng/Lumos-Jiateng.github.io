---
layout: default
title: Jiateng Liu, Personal Home Page
---
<div class="blurb">
	
	<h1>Welcome to Jiateng Home Page !</h1>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              
              <h3>
              Hi there ! Welcome to my personal homepage!
              </h3>
              <p> I'm Jiateng Liu (刘嘉腾), a second-year MS student at the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign (UIUC) </a> under the guidance of Prof. <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>. Previously, I earned my bachelor's degree in Computer Science from <a href="https://www.zju.edu.cn/">Zhejiang University</a>. 
              </p>

              <p> 
                I'm interested in the <strong>Science</strong> of NLP while keeps an eye on its <strong>Applications</strong>.
              </p>

              <p> 
                <strong>Science:</strong> I am deeply committed to the rigorous study of large models. I strive to enhance their interpretability and effectiveness. As we move into the next decade, I am excited to contribute to the evolution of AI—developing systems that not only perform complex tasks but are also interpretable and genuinely beneficial to humanity.
              </p>
                
              <p> 
                <strong>Applications:</strong> I believe that AI must transcend theoretical prowess to substantial improvements in daily life. I keep an eye on the applications of NLP in Science and looking forward to the next 'ChatGPT' moment.
              </p>
              
            
            </td>

            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="images/graduation.jpg">
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              
              <h2>
                Research
              </h2>
              
              <p> My research interests are organized into three key areas: </p> 
              
              <p> 1. <strong>The Physics and Interpretability of Language Models:</strong> I am intrigued by the underlying "physics" of Large Language Models (LLMs), focusing on how they absorb knowledge, process information, and make predictions. A major part of my research is improving the efficiency and accuracy of updating pretrained LLMs, ensuring that their knowledge remains robust, consistent, and up-to-date while minimizing costs and time. Additionally, I am deeply interested in the interpretability of LLMs—understanding how they represent and manipulate information internally to provide more transparent and trustworthy AI systems. </p>
              
              <p> 2. <strong>Multi-Modal Representation Learning and Multi-Media Foundational Models:</strong> My work in this area centers on designing new paradigms for multi-modal interactions and deriving empirical scaling laws for multi-modal foundational models. I focus particularly on video understanding and generation, aiming to seamlessly integrate language, visual, and temporal modalities. A key aspect of my research is exploring how multimodal interactions are learned, including the mechanisms by which information flows and aligns across modalities to create cohesive representations. I also investigate protocols for efficient and complete multimodal interactions, ensuring that each modality contributes optimally to the task at hand while minimizing redundancy and maximizing interpretability. My goal is to develop systems capable of effectively analyzing and generating content across diverse modalities, with applications in tasks such as video captioning, video-based reasoning, and video synthesis, thereby advancing the capabilities of multi-modal AI. </p> 
              
              <p> 3. <strong>NLP and Multi-Disciplinary Science for Social Good:</strong> I use NLP to tackle challenges in social and scientific domains. For example, I analyze social media data to study the spread of misinformation and its societal impacts, helping to develop tools that counteract disinformation. Additionally, I explore how NLP can contribute to scientific advancements, such as facilitating drug design and interpreting neural signals. These interdisciplinary applications demonstrate the transformative potential of NLP in both improving lives and advancing science.</p>
                
              <p>
                For more on my future vision and projects, feel free to visit my blog or reach out for a discussion.
              </p>

              <p>
                I am currently seeking a Ph.D. position starting in Fall 2025! 
              </p>
              
            
            </td>

          </tr>
        </table>


  <div class="publications-section">
    <h2>Peer-reviewed Publications</h2>
    
    <!-- Publication 1: EVEDIT -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/1.jpg" alt="EVEDIT: Event-based Knowledge Editing" onclick="openImageModal('images/1.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag emnlp">EMNLP-2024</span>
          <span class="publication-tag">Knowledge Editing</span>
        </div>
        <h3 class="publication-title">EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Pengfei Yu*, Yuji Zhang, Sha Li, Zixuan Zhang, Ruhi Sarikaya, Kevin Small, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP-2024 Main Conference)</div>
        <div class="publication-links">
          <a href="https://aclanthology.org/2024.emnlp-main.282/" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 2: Procedure Planning -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/2.jpg" alt="A Language First Approach for Procedure Planning" onclick="openImageModal('images/2.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag acl">ACL-2023</span>
          <span class="publication-tag">Procedure Planning</span>
        </div>
        <h3 class="publication-title">A Language First Approach for Procedure Planning</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Sha Li*, Zhenhailong Wang, Manling Li, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023 Findings)</div>
        <div class="publication-links">
          <a href="https://aclanthology.org/2023.findings-acl.122.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 3: PropaInsight -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/3.jpg" alt="PropaInsight: Toward Deeper Understanding of Propaganda" onclick="openImageModal('images/3.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag coling">COLING-2025</span>
          <span class="publication-tag">Propaganda Analysis</span>
        </div>
        <h3 class="publication-title">PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent</h3>
        <div class="publication-authors"><strong>Jiateng Liu*</strong>, Lin Ai*, Zizhou Liu, Payam Karisani, Zheng Hui, May Fung, Preslav Nakov, Julia Hirschberg, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 29th International Conference on Computational Linguistics (COLING 2025)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2409.18997" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 4: Code Survey -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/5.jpg" alt="If LLM Is the Wizard, Then Code Is the Wand" onclick="openImageModal('images/5.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag iclr">ICLR-2024</span>
          <span class="publication-tag">Survey</span>
        </div>
        <h3 class="publication-title">If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents</h3>
        <div class="publication-authors">Ke Yang*, <strong>Jiateng Liu*</strong>, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai</div>
        <div class="publication-venue">ICLR 2024 Workshop (submitting to ACM Computing Survey)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2401.00812.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 5: MINT -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/4.jpg" alt="MINT: Evaluating LLMs in Multi-turn Interaction" onclick="openImageModal('images/4.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag iclr">ICLR-2024</span>
          <span class="publication-tag">LLM Evaluation</span>
        </div>
        <h3 class="publication-title">MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</h3>
        <div class="publication-authors">Xingyao Wang, Zihan Wang, <strong>Jiateng Liu</strong>, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</div>
        <div class="publication-venue">Proceedings of the 12th International Conference on Learning Representations (ICLR 2024)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2309.10691.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 6: CurveCloudNet -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/6.jpg" alt="CurveCloudNet: Processing Point Clouds" onclick="openImageModal('images/6.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag cvpr">CVPR-2024</span>
          <span class="publication-tag">3D Vision</span>
        </div>
        <h3 class="publication-title">CurveCloudNet: Processing Point Clouds with 1D Structure</h3>
        <div class="publication-authors">Colton Stearns, <strong>Jiateng Liu</strong>, Davis Rempe, Despoina Paschalidou, Jeong Joon Park, Sebastien Mascha, Leonidas J. Guibas</div>
        <div class="publication-venue">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2303.12050.pdf" class="publication-link">Paper</a>
        </div>
      </div>
    </div>

    <!-- Publication 7: Knowledge Overshadowing -->
    <div class="publication-card">
      <div class="publication-image">
        <img src="images/7.jpg" alt="Knowledge overshadowing causes amalgamated hallucination" onclick="openImageModal('images/7.jpg')">
      </div>
      <div class="publication-content">
        <div class="publication-tags">
          <span class="publication-tag arxiv">arXiv</span>
          <span class="publication-tag">Hallucination</span>
        </div>
        <h3 class="publication-title">Knowledge overshadowing causes amalgamated hallucination in large language models</h3>
        <div class="publication-authors">Yuji Zhang, Sha Li, <strong>Jiateng Liu</strong>, Pengfei Yu, Yi R Fung, Jing Li, Manling Li, Heng Ji</div>
        <div class="publication-venue">arXiv preprint</div>
        <div class="publication-links">
          <a href="https://arxiv.org/pdf/2407.08039" class="publication-link">Paper</a>
        </div>
      </div>
    </div>
    
    <div style="margin-top: 40px; text-align: center; font-style: italic; color: #6b7280;">
      For the full list of publications, please visit my <a href="https://scholar.google.com/citations?user=a5_PfQQAAAAJ&hl" style="color: #3b82f6; text-decoration: none;">Google Scholar page</a>.
    </div>
  </div>

  <!-- Under-review Pre-prints Section -->
  <div class="preprints-section">
    <h2>Under-review Pre-prints</h2>
    <div class="preprints-grid">
      <!-- Preprint 1 -->
      <div class="preprint-card">
        <div class="preprint-image">
          <img src="images/1.jpg" alt="Preprint 1" onclick="openImageModal('images/1.jpg')">
        </div>
        <div class="preprint-content">
          <h3 class="preprint-title">EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation</h3>
          <div class="preprint-authors"><strong>Jiateng Liu*</strong>, Pengfei Yu*, Yuji Zhang, Sha Li, Zixuan Zhang, Ruhi Sarikaya, Kevin Small, Heng Ji</div>
          <div class="preprint-venue">Under Review at EMNLP 2024</div>
          <div class="preprint-links">
            <a href="https://aclanthology.org/2024.emnlp-main.282/" class="preprint-link">Paper</a>
          </div>
        </div>
      </div>

      <!-- Preprint 2 -->
      <div class="preprint-card">
        <div class="preprint-image">
          <img src="images/2.jpg" alt="Preprint 2" onclick="openImageModal('images/2.jpg')">
        </div>
        <div class="preprint-content">
          <h3 class="preprint-title">A Language First Approach for Procedure Planning</h3>
          <div class="preprint-authors"><strong>Jiateng Liu*</strong>, Sha Li*, Zhenhailong Wang, Manling Li, Heng Ji</div>
          <div class="preprint-venue">Under Review at ACL 2023</div>
          <div class="preprint-links">
            <a href="https://aclanthology.org/2023.findings-acl.122.pdf" class="preprint-link">Paper</a>
          </div>
        </div>
      </div>

      <!-- Preprint 3 -->
      <div class="preprint-card">
        <div class="preprint-image">
          <img src="images/3.jpg" alt="Preprint 3" onclick="openImageModal('images/3.jpg')">
        </div>
        <div class="preprint-content">
          <h3 class="preprint-title">PropaInsight: Toward Deeper Understanding of Propaganda</h3>
          <div class="preprint-authors"><strong>Jiateng Liu*</strong>, Lin Ai*, Zizhou Liu, Payam Karisani, Zheng Hui, May Fung, Preslav Nakov, Julia Hirschberg, Heng Ji</div>
          <div class="preprint-venue">Under Review at COLING 2025</div>
          <div class="preprint-links">
            <a href="https://arxiv.org/pdf/2409.18997" class="preprint-link">Paper</a>
          </div>
        </div>
      </div>
    </div>
  </div>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h2>
                Research Intern experience<br>
              </h2>
              <h3>
                Nov 2022 - March 2023 <a href="https://www.stanford.edu/">[Stanford University]</a>
              </h3>
              <ul style="list-style-type: circle;">
                <li><b>Research Intern.</b></li>
                <li><b>Mentor:</b> Prof. Leonidas Guibas, Prof. Yanchao Yang, Colton Stearns</li>
                <li><b>Focus:</b> 3D reconstruction with curve data</li>
              </ul>

              <h3>
                June 2022 - Dec 2022 <a href="https://illinois.edu/">[University of Illinois Urbana Champaign]</a>
              </h3>
              <ul style="list-style-type: circle;">
                <li><b>Research Intern.</b></li>
                <li><b>Mentor:</b> Prof. Heng Ji, Sha Li, Manling Li</li>
                <li><b>Focus:</b> Language side approaches for Procedure Planning </li>
              </ul>

              <h3>
                June 2022 - Dec 2022 <a href="https://www.zju.edu.cn/">[Zhejiang University]</a>
              </h3>
              <ul style="list-style-type: circle;">
                <li><b>Research Intern.</b></li>
                <li><b>Mentor:</b> Prof. Mingli Song, Prof. Zunlei Feng, Ya Zhao</li>
                <li><b>Focus:</b> Make Transformers efficient </li>
              </ul>

              <h3>
                Sep 2021 - Dec 2021 <a href="https://www.zju.edu.cn/">[Zhejiang University]</a>
              </h3>
              <ul style="list-style-type: circle;">
                <li><b>Research Intern.</b></li>
                <li><b>Mentor:</b> Prof. Zicheng Liu, Prof. Mingli Song</li>
                <li><b>Focus:</b> 3D Human mesh reconstruction </li>
              </ul>

          </tr>
        </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h2>
                Assistanship<br>
              </h2>
              <h3>
                2023.8 - 2023.12 <br> Teaching assistant for CS440 at UIUC.
              </h3>

              <h3>
                2023.12 - now <br> Research assistant of Prof. Heng Ji.
              </h3>

          </tr>
        </table>
	
	</div><!-- /.blurb -->

<!-- Image Modal -->
<div id="imageModal" class="image-modal" onclick="closeImageModal()">
    <span class="image-modal-close" onclick="closeImageModal()">&times;</span>
    <img class="image-modal-content" id="modalImage">
</div>

<script>
function openImageModal(imageSrc) {
    const modal = document.getElementById('imageModal');
    const modalImg = document.getElementById('modalImage');
    modal.style.display = 'block';
    modalImg.src = imageSrc;
}

function closeImageModal() {
    document.getElementById('imageModal').style.display = 'none';
}

// Close modal when clicking outside the image
document.getElementById('imageModal').addEventListener('click', function(e) {
    if (e.target === this) {
        closeImageModal();
    }
});

// Close modal with Escape key
document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape') {
        closeImageModal();
    }
});
</script>
